{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e6fa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7acb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (4.10.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/10/56/8288d1813a68c1e0638515dbb777fce6d87d0d240e683216f956145310e6/selenium-4.11.2-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.11.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.11.2-py3-none-any.whl (7.2 MB)\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/7.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------- 0.1/7.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------- 0.1/7.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------- 0.1/7.2 MB 4.8 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.1/7.2 MB 655.8 kB/s eta 0:00:11\n",
      "    --------------------------------------- 0.2/7.2 MB 655.4 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.3/7.2 MB 830.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.4/7.2 MB 1.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/7.2 MB 1.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.6/7.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.8/7.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.9/7.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.3/7.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.3/7.2 MB 1.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.5/7.2 MB 1.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.6/7.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.8/7.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.9/7.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.1/7.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.3/7.2 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.4/7.2 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.6/7.2 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.6/7.2 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.6/7.2 MB 2.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.7/7.2 MB 2.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.9/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.1/7.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.3/7.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.4/7.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.6/7.2 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.7/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.9/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.0/7.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.3/7.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.5/7.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.6/7.2 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.9/7.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.1/7.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.4/7.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.6/7.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.7/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.9/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.1/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.4/7.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.7/7.2 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.9/7.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.2/7.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.2/7.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.2/7.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.2/7.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.2/7.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.2/7.2 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 4.10.0\n",
      "    Uninstalling selenium-4.10.0:\n",
      "      Successfully uninstalled selenium-4.10.0\n",
      "Successfully installed selenium-4.11.2\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d942f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: requests in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from webdriver-manager) (4.65.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaikh\\anaconda3\\lib\\site-packages (from tqdm->webdriver-manager) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa29b60",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce759a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By #importing inbulit class By\n",
    "import warnings                             #To ignore any sort of warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6387af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open automated wikipedia driver page\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2be6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_list1 = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "rank=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[1]\")\n",
    "for i in rank:\n",
    "    rank_list1.append(i.text)\n",
    "    \n",
    "name=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[2]\")\n",
    "for i in name:\n",
    "    name_list.append(i.text)\n",
    "\n",
    "\n",
    "artist=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[3]\")\n",
    "for i in artist:\n",
    "    artist_list.append(i.text)\n",
    "\n",
    "date=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[5]\")\n",
    "for i in date:\n",
    "    upload_date_list.append(i.text)\n",
    "    \n",
    "views=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\")\n",
    "for i in views:\n",
    "    views_list.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d43e54fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "print(len(rank_list1),len(name_list),len(artist_list),len(upload_date_list),len(views_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fd4f9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - children's songs</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>13.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[16]</td>\n",
       "      <td>LooLoo Kids - nursery rhymes</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[18]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[21]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[27]</td>\n",
       "      <td>ChuChu TV - children's songs</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[28]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[29]</td>\n",
       "      <td>Miroshka TV - children's songs</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[30]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[35]</td>\n",
       "      <td>Get Movies - children's songs</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[36]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[38]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[39]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Sorry\"[42]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[43]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Thinking Out Loud\"[44]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[45]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Dark Horse\"[46]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Perfect\"[47]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Faded\"[48]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[49]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[50]</td>\n",
       "      <td>Kiddiestv Hindi - children's songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Girls Like You\"[51]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Bailando\"[52]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Lean On\"[53]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[6]   \n",
       "1    2.                                   \"Despacito\"[9]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[16]   \n",
       "3    4.                                  \"Bath Song\"[17]   \n",
       "4    5.                               \"Shape of You\"[18]   \n",
       "5    6.                              \"See You Again\"[21]   \n",
       "6    7.                          \"Wheels on the Bus\"[26]   \n",
       "7    8.                \"Phonics Song with Two Words\"[27]   \n",
       "8    9.                                \"Uptown Funk\"[28]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[29]   \n",
       "10  11.                              \"Gangnam Style\"[30]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[35]   \n",
       "12  13.                             \"Dame Tu Cosita\"[36]   \n",
       "13  14.                                     \"Axel F\"[37]   \n",
       "14  15.                                      \"Sugar\"[38]   \n",
       "15  16.                                       \"Roar\"[39]   \n",
       "16  17.                             \"Counting Stars\"[40]   \n",
       "17  18.                        \"Baa Baa Black Sheep\"[41]   \n",
       "18  19.                                      \"Sorry\"[42]   \n",
       "19  20.           \"Waka Waka (This Time for Africa)\"[43]   \n",
       "20  21.                          \"Thinking Out Loud\"[44]   \n",
       "21  22.                             \"Lakdi Ki Kathi\"[45]   \n",
       "22  23.                                 \"Dark Horse\"[46]   \n",
       "23  24.                                    \"Perfect\"[47]   \n",
       "24  25.                                      \"Faded\"[48]   \n",
       "25  26.                                 \"Let Her Go\"[49]   \n",
       "26  27.          \"Humpty the train on a fruits ride\"[50]   \n",
       "27  28.                             \"Girls Like You\"[51]   \n",
       "28  29.                                   \"Bailando\"[52]   \n",
       "29  30.                                    \"Lean On\"[53]   \n",
       "\n",
       "                                    Artist        Upload Date  Views  \n",
       "0   Pinkfong Baby Shark - children's songs      June 17, 2016  13.18  \n",
       "1                               Luis Fonsi   January 12, 2017   8.23  \n",
       "2             LooLoo Kids - nursery rhymes    October 8, 2016   6.76  \n",
       "3               Cocomelon - nursery rhymes        May 2, 2018   6.33  \n",
       "4                               Ed Sheeran   January 30, 2017   6.05  \n",
       "5                              Wiz Khalifa      April 6, 2015   5.98  \n",
       "6               Cocomelon - nursery rhymes       May 24, 2018   5.46  \n",
       "7             ChuChu TV - children's songs      March 6, 2014   5.42  \n",
       "8                              Mark Ronson  November 19, 2014   5.00  \n",
       "9           Miroshka TV - children's songs  February 27, 2018   4.94  \n",
       "10                                     Psy      July 15, 2012   4.86  \n",
       "11           Get Movies - children's songs   January 31, 2012   4.55  \n",
       "12                               El Chombo      April 5, 2018   4.41  \n",
       "13                              Crazy Frog      June 16, 2009   4.00  \n",
       "14                                Maroon 5   January 14, 2015   3.91  \n",
       "15                              Katy Perry  September 5, 2013   3.84  \n",
       "16                             OneRepublic       May 31, 2013   3.84  \n",
       "17              Cocomelon - nursery rhymes      June 25, 2018   3.73  \n",
       "18                           Justin Bieber   October 22, 2015   3.69  \n",
       "19                                 Shakira       June 4, 2010   3.68  \n",
       "20                              Ed Sheeran    October 7, 2014   3.63  \n",
       "21                            Jingle Toons      June 14, 2018   3.63  \n",
       "22                              Katy Perry  February 20, 2014   3.56  \n",
       "23                              Ed Sheeran   November 9, 2017   3.51  \n",
       "24                             Alan Walker   December 3, 2015   3.49  \n",
       "25                               Passenger      July 25, 2012   3.48  \n",
       "26      Kiddiestv Hindi - children's songs   January 26, 2018   3.51  \n",
       "27                                Maroon 5       May 31, 2018   3.45  \n",
       "28                        Enrique Iglesias     April 11, 2014   3.43  \n",
       "29                             Major Lazer     March 22, 2015   3.43  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.DataFrame({\"Rank\":rank_list1,\"Name\":name_list,\"Artist\":artist_list,\"Upload Date\":upload_date_list,\"Views\":views_list})\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75a88",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7895fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By #importing inbulit class By\n",
    "import warnings                             #To ignore any sort of warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed48980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open automated wikipedia driver page\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.bcci.tv/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42c6f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "international_element=driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a')\n",
    "international_element.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce047777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the details \n",
    "Match_title=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n",
    "\n",
    "match_title=driver.find_elements(By.XPATH,'//*[@class=\"match-card ng-scope\"]/div[3]/div/span[1]')\n",
    "for i in match_title:\n",
    "    Match_title.append(i.text)\n",
    "    \n",
    "series=driver.find_elements(By.XPATH,'//*[@class=\"match-card ng-scope\"]/div[1]/h5')\n",
    "for i in series:\n",
    "    Series.append(i.text)\n",
    "    \n",
    "place=driver.find_elements(By.XPATH,'//*[@class=\"match-card ng-scope\"]/div[3]/div/span[2]')\n",
    "for i in place:\n",
    "    Place.append(i.text)\n",
    "    \n",
    "date=driver.find_elements(By.XPATH,'//*[@class=\"match-card ng-scope\"]/div[1]/div/div[1]')\n",
    "for i in date:\n",
    "    Date.append(i.text)\n",
    "    \n",
    "time=driver.find_elements(By.XPATH,'//*[@class=\"match-card ng-scope\"]/div[1]/div/div[2]')\n",
    "for i in time:\n",
    "    Time.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0c77e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 8 8 8\n"
     ]
    }
   ],
   "source": [
    "print(len(Match_title),len(Series),len(Place),len(Date),len(Time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e29560e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>INDIA TOUR OF IRELAND 2023</td>\n",
       "      <td>The Village,</td>\n",
       "      <td>18 AUG 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I -</td>\n",
       "      <td>INDIA TOUR OF IRELAND 2023</td>\n",
       "      <td>The Village,</td>\n",
       "      <td>20 AUG 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd T20I -</td>\n",
       "      <td>INDIA TOUR OF IRELAND 2023</td>\n",
       "      <td>The Village,</td>\n",
       "      <td>23 AUG 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>ASIA CUP 2023</td>\n",
       "      <td>Pallekele International Cricket Stadium,</td>\n",
       "      <td>2 SEP 2023</td>\n",
       "      <td>10:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>ASIA CUP 2023</td>\n",
       "      <td>Pallekele International Cricket Stadium,</td>\n",
       "      <td>4 SEP 2023</td>\n",
       "      <td>10:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Punjab Cricket Association IS Bindra Stadium,</td>\n",
       "      <td>22 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Holkar Cricket Stadium,</td>\n",
       "      <td>24 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd ODI -</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Saurashtra Cricket Association Stadium,</td>\n",
       "      <td>27 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match title                           Series  \\\n",
       "0  1st T20I -       INDIA TOUR OF IRELAND 2023   \n",
       "1  2nd T20I -       INDIA TOUR OF IRELAND 2023   \n",
       "2  3rd T20I -       INDIA TOUR OF IRELAND 2023   \n",
       "3   1st ODI -                    ASIA CUP 2023   \n",
       "4   2nd ODI -                    ASIA CUP 2023   \n",
       "5   1st ODI -  AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "6   2nd ODI -  AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "7   3rd ODI -  AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "\n",
       "                                           Place         Date          Time  \n",
       "0                                   The Village,  18 AUG 2023   7:30 PM IST  \n",
       "1                                   The Village,  20 AUG 2023   7:30 PM IST  \n",
       "2                                   The Village,  23 AUG 2023   7:30 PM IST  \n",
       "3       Pallekele International Cricket Stadium,   2 SEP 2023  10:00 AM IST  \n",
       "4       Pallekele International Cricket Stadium,   4 SEP 2023  10:00 AM IST  \n",
       "5  Punjab Cricket Association IS Bindra Stadium,  22 SEP 2023   1:30 PM IST  \n",
       "6                        Holkar Cricket Stadium,  24 SEP 2023   1:30 PM IST  \n",
       "7        Saurashtra Cricket Association Stadium,  27 SEP 2023   1:30 PM IST  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.DataFrame({\"Match title\":Match_title,\"Series\":Series,\"Place\":Place,\"Date\":Date,\"Time\":Time})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a206e",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e90efe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By #importing inbulit class By\n",
    "import warnings                             #To ignore any sort of warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f607bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the automated page of the website\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"http://statisticstimes.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f269f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the economy page of the website\n",
    "economy=driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/button')\n",
    "economy.click()\n",
    "\n",
    "#for finding the element by hover on \n",
    "India=driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/div/a[3]')\n",
    "India.click()\n",
    "\n",
    "#clicking of the GDP states\n",
    "GDP=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "GDP.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0750cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping the details of the page\n",
    "Rank=[]\n",
    "State=[]\n",
    "GSDP_18_19_current=[]\n",
    "GSDP_19_20_current=[]\n",
    "share_18_19=[]\n",
    "GDP=[]\n",
    "\n",
    "rank_element=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "for i in rank_element:\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "state=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "for i in state:\n",
    "    State.append(i.text)\n",
    "    \n",
    "gsdp_18_19_current=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "for i in gsdp_18_19_current:\n",
    "    GSDP_18_19_current.append(i.text)\n",
    "    \n",
    "gsdp_19_20=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "for i in gsdp_19_20:\n",
    "    GSDP_19_20_current.append(i.text)\n",
    "    \n",
    "share=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "for i in share:\n",
    "    share_18_19.append(i.text)\n",
    "    \n",
    "gdp=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "for i in gdp:\n",
    "    GDP.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a61f1ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 33 33 33 33\n"
     ]
    }
   ],
   "source": [
    "print(len(Rank),len(State),len(GSDP_18_19_current),len(GSDP_19_20_current),len(share_18_19),len(GDP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c25e6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP_18_19_current</th>\n",
       "      <th>GSDP_19_20_current</th>\n",
       "      <th>share_18_19</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP_18_19_current GSDP_19_20_current  \\\n",
       "0     1                Maharashtra          2,632,792                  -   \n",
       "1     2                 Tamil Nadu          1,630,208          1,845,853   \n",
       "2     3              Uttar Pradesh          1,584,764          1,687,818   \n",
       "3     4                    Gujarat          1,502,899                  -   \n",
       "4     5                  Karnataka          1,493,127          1,631,977   \n",
       "5     6                West Bengal          1,089,898          1,253,832   \n",
       "6     7                  Rajasthan            942,586          1,020,989   \n",
       "7     8             Andhra Pradesh            862,957            972,782   \n",
       "8     9                  Telangana            861,031            969,604   \n",
       "9    10             Madhya Pradesh            809,592            906,672   \n",
       "10   11                     Kerala            781,653                  -   \n",
       "11   12                      Delhi            774,870            856,112   \n",
       "12   13                    Haryana            734,163            831,610   \n",
       "13   14                      Bihar            530,363            611,804   \n",
       "14   15                     Punjab            526,376            574,760   \n",
       "15   16                     Odisha            487,805            521,275   \n",
       "16   17                      Assam            315,881                  -   \n",
       "17   18               Chhattisgarh            304,063            329,180   \n",
       "18   19                  Jharkhand            297,204            328,598   \n",
       "19   20                Uttarakhand            245,895                  -   \n",
       "20   21            Jammu & Kashmir            155,956                  -   \n",
       "21   22           Himachal Pradesh            153,845            165,472   \n",
       "22   23                        Goa             73,170             80,449   \n",
       "23   24                    Tripura             49,845             55,984   \n",
       "24   25                 Chandigarh             42,114                  -   \n",
       "25   26                 Puducherry             34,433             38,253   \n",
       "26   27                  Meghalaya             33,481             36,572   \n",
       "27   28                     Sikkim             28,723             32,496   \n",
       "28   29                    Manipur             27,870             31,790   \n",
       "29   30                   Nagaland             27,283                  -   \n",
       "30   31          Arunachal Pradesh             24,603                  -   \n",
       "31   32                    Mizoram             22,287             26,503   \n",
       "32   33  Andaman & Nicobar Islands                  -                  -   \n",
       "\n",
       "   share_18_19      GDP  \n",
       "0       13.94%  399.921  \n",
       "1        8.63%  247.629  \n",
       "2        8.39%  240.726  \n",
       "3        7.96%  228.290  \n",
       "4        7.91%  226.806  \n",
       "5        5.77%  165.556  \n",
       "6        4.99%  143.179  \n",
       "7        4.57%  131.083  \n",
       "8        4.56%  130.791  \n",
       "9        4.29%  122.977  \n",
       "10       4.14%  118.733  \n",
       "11       4.10%  117.703  \n",
       "12       3.89%  111.519  \n",
       "13       2.81%   80.562  \n",
       "14       2.79%   79.957  \n",
       "15       2.58%   74.098  \n",
       "16       1.67%   47.982  \n",
       "17       1.61%   46.187  \n",
       "18       1.57%   45.145  \n",
       "19       1.30%   37.351  \n",
       "20       0.83%   23.690  \n",
       "21       0.81%   23.369  \n",
       "22       0.39%   11.115  \n",
       "23       0.26%    7.571  \n",
       "24       0.22%    6.397  \n",
       "25       0.18%    5.230  \n",
       "26       0.18%    5.086  \n",
       "27       0.15%    4.363  \n",
       "28       0.15%    4.233  \n",
       "29       0.14%    4.144  \n",
       "30       0.13%    3.737  \n",
       "31       0.12%    3.385  \n",
       "32           -        -  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=pd.DataFrame({\"Rank\":Rank,\"State\":State,\"GSDP_18_19_current\":GSDP_18_19_current,\"GSDP_19_20_current\":GSDP_19_20_current,\"share_18_19\":share_18_19,\"GDP\":GDP})\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1a53c",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd620782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By #importing inbulit class By\n",
    "import warnings                             #To ignore any sort of warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b2d5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the automated page \n",
    "driver=webdriver.Chrome()\n",
    "driver.get('https://github.com/')\n",
    "\n",
    "#finding the element by Hover on\n",
    "open_source_hover=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button')\n",
    "\n",
    "#creating the Actionchains instance\n",
    "actions=ActionChains(driver)\n",
    "# Move the cursor to the hover element\n",
    "actions.move_to_element(open_source_hover).perform()\n",
    "\n",
    "#after hover on open source then clicking on repositories\n",
    "repo_trending=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "repo_trending.click()                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4458a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping all repositories URL\n",
    "repository_url=[]\n",
    "url=driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]/a')\n",
    "for i in url:\n",
    "    repository_url.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd26477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repository_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a06fae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Repository_title=[]\n",
    "Repository_description=[]\n",
    "Countributors_count=[]\n",
    "Language_used=[]\n",
    "\n",
    "repository=driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]/a')\n",
    "for i in repository:\n",
    "    Repository_title.append(i.text.split('/')[1])\n",
    "    \n",
    "description=driver.find_elements(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]')\n",
    "for i in description:\n",
    "    Repository_description.append(i.text)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65aea05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(Repository_title),len(Repository_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21f0bb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository title</th>\n",
       "      <th>Repository Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-gpt</td>\n",
       "      <td>A self-hosted, offline, ChatGPT-like chatbot. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CoDeF</td>\n",
       "      <td>Official PyTorch implementation of CoDeF: Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stable-diffusion.cpp</td>\n",
       "      <td>Stable Diffusion in pure C/C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python-100-Days</td>\n",
       "      <td>Python - 100天从新手到大师</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VPet</td>\n",
       "      <td>虚拟桌宠模拟器 一个开源的桌宠软件, 可以内置到任何WPF应用程序</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DevOpsGPT</td>\n",
       "      <td>Multi agent system for AI-driven software deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cal.com</td>\n",
       "      <td>Scheduling infrastructure for absolutely every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grasscutter</td>\n",
       "      <td>A server software reimplementation for a certa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>awesome-python</td>\n",
       "      <td>A curated list of awesome Python frameworks, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OldTweetDeck</td>\n",
       "      <td>Returns old TweetDeck, for free!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>serde</td>\n",
       "      <td>Serialization framework for Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>biomes-game</td>\n",
       "      <td>Biomes is an open source sandbox MMORPG built ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AnimatedDrawings</td>\n",
       "      <td>Code to accompany \"A Method for Animating Chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dio-lab-open-source</td>\n",
       "      <td>Repositório do lab Contribuindo em um Projeto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>documenso</td>\n",
       "      <td>The Open Source DocuSign Alternative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bringing-Old-Photos-Back-to-Life</td>\n",
       "      <td>Bringing Old Photo Back to Life (CVPR 2020 oral)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>organicmaps</td>\n",
       "      <td>🍃 Organic Maps is a free Android &amp; iOS offline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>OpenBBTerminal</td>\n",
       "      <td>Investment Research for Everyone, Everywhere.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>free-programming-books</td>\n",
       "      <td>📚 Freely available programming books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GodMode</td>\n",
       "      <td>AI Chat Browser: Fast, Full webapp access to C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GPT-vup</td>\n",
       "      <td>GPT-vup BIliBili | 抖音 | AI | 虚拟主播</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>awesome-english-ebooks</td>\n",
       "      <td>经济学人(含音频)、纽约客、卫报、连线、大西洋月刊等英语杂志免费下载,支持epub、mobi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>supervision</td>\n",
       "      <td>We write your reusable computer vision tools. 💜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>aseprite</td>\n",
       "      <td>Animated sprite editor &amp; pixel art tool (Windo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>quivr</td>\n",
       "      <td>🧠 Your Second Brain supercharged by Generative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Repository title  \\\n",
       "0                           llama-gpt   \n",
       "1                               CoDeF   \n",
       "2                stable-diffusion.cpp   \n",
       "3                     Python-100-Days   \n",
       "4                                VPet   \n",
       "5                           DevOpsGPT   \n",
       "6                             cal.com   \n",
       "7                         Grasscutter   \n",
       "8                      awesome-python   \n",
       "9                        OldTweetDeck   \n",
       "10                              serde   \n",
       "11                        biomes-game   \n",
       "12                   AnimatedDrawings   \n",
       "13                dio-lab-open-source   \n",
       "14                          documenso   \n",
       "15   Bringing-Old-Photos-Back-to-Life   \n",
       "16                        organicmaps   \n",
       "17                     OpenBBTerminal   \n",
       "18             free-programming-books   \n",
       "19                            GodMode   \n",
       "20                            GPT-vup   \n",
       "21             awesome-english-ebooks   \n",
       "22                        supervision   \n",
       "23                           aseprite   \n",
       "24                              quivr   \n",
       "\n",
       "                               Repository Description  \n",
       "0   A self-hosted, offline, ChatGPT-like chatbot. ...  \n",
       "1   Official PyTorch implementation of CoDeF: Cont...  \n",
       "2                      Stable Diffusion in pure C/C++  \n",
       "3                                 Python - 100天从新手到大师  \n",
       "4                   虚拟桌宠模拟器 一个开源的桌宠软件, 可以内置到任何WPF应用程序  \n",
       "5   Multi agent system for AI-driven software deve...  \n",
       "6   Scheduling infrastructure for absolutely every...  \n",
       "7   A server software reimplementation for a certa...  \n",
       "8   A curated list of awesome Python frameworks, l...  \n",
       "9                    Returns old TweetDeck, for free!  \n",
       "10                   Serialization framework for Rust  \n",
       "11  Biomes is an open source sandbox MMORPG built ...  \n",
       "12  Code to accompany \"A Method for Animating Chil...  \n",
       "13  Repositório do lab Contribuindo em um Projeto ...  \n",
       "14              The Open Source DocuSign Alternative.  \n",
       "15   Bringing Old Photo Back to Life (CVPR 2020 oral)  \n",
       "16  🍃 Organic Maps is a free Android & iOS offline...  \n",
       "17      Investment Research for Everyone, Everywhere.  \n",
       "18               📚 Freely available programming books  \n",
       "19  AI Chat Browser: Fast, Full webapp access to C...  \n",
       "20                  GPT-vup BIliBili | 抖音 | AI | 虚拟主播  \n",
       "21  经济学人(含音频)、纽约客、卫报、连线、大西洋月刊等英语杂志免费下载,支持epub、mobi...  \n",
       "22    We write your reusable computer vision tools. 💜  \n",
       "23  Animated sprite editor & pixel art tool (Windo...  \n",
       "24  🧠 Your Second Brain supercharged by Generative...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4=pd.DataFrame({\"Repository title\":Repository_title,\"Repository Description\":Repository_description})\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa6686f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in repository_url: #loop for every repository in list\n",
    "    driver.get(url)\n",
    "    time.sleep(10) \n",
    "    \n",
    "    try:\n",
    "        Language=WebDriverWait(driver,10).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"repo-content-pjax-container\"]/div/div/div[2]/div[2]/div/div[4]/div/ul/li[1]/a/span[1]')))\n",
    "        Language_used.append(Language.text)\n",
    "    except (TimeoutException,NoSuchElementException):\n",
    "        Language_used.append(\"-\") \n",
    "    \n",
    "    try:\n",
    "        contributors=WebDriverWait(driver,10).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"repo-content-pjax-container\"]/div/div/div[2]/div[2]/div/div[3]/div/h2/a/span')))\n",
    "        Countributors_count.append(contributors.text)\n",
    "    except (TimeoutException,NoSuchElementException):\n",
    "        Countributors_count.append(\"-\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71e77e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 26 26\n"
     ]
    }
   ],
   "source": [
    "print(len(Repository_title),len(Repository_title),len(Language_used),len(Countributors_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "33f82f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TypeScript', 'TypeScript', '-', '-', '-', '@LorisYounger\\n/ VPet.Plugin.Demo', '-', '-', '-', 'Python', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '@ftdebugger\\n/ whole-npm', '-', '-', '-', '-', 'quivr'] ['4', '4', '-', '-', '-', '-', '-', '830', '-', '401', '-', '619k', '-', '-', '-', '-', '-', '-', '17', '7', '-', '-', '-', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "print((Language_used),(Countributors_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbdeda",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afa1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By #importing inbuilt class By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "779a28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the automated page\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"http://www.billboard.com/\")\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad3fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clicking on the charts\n",
    "charts_tags=driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "charts_tags.click()\n",
    "\n",
    "open_chart=driver.find_element(By.XPATH,'//div[@class=\"lrv-u-flex lrv-u-justify-content-center\"]/a')\n",
    "open_chart.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cd84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the details \n",
    "\n",
    "Song_Name=[]\n",
    "Artist_name=[]\n",
    "Last_week_rank=[]\n",
    "Peak_rank=[]\n",
    "Weeks_on_board=[]\n",
    "\n",
    "Name=driver.find_elements(By.XPATH,'//div[@class=\"chart-results-list // lrv-u-padding-t-150 lrv-u-padding-t-050@mobile-max\"]/div/ul/li[4]/ul/li[1]/h3')\n",
    "for i in Name:\n",
    "    Song_Name.append(i.text)\n",
    "    \n",
    "artist=driver.find_elements(By.XPATH,'//div[@class=\"chart-results-list // lrv-u-padding-t-150 lrv-u-padding-t-050@mobile-max\"]/div/ul/li[4]/ul/li[1]/span')\n",
    "for i in artist:\n",
    "    Artist_name.append(i.text)\n",
    "    \n",
    "lastrank=driver.find_elements(By.XPATH,'//div[@class=\"chart-results-list // lrv-u-padding-t-150 lrv-u-padding-t-050@mobile-max\"]/div/ul/li[4]/ul/li[4]/span')\n",
    "for i in lastrank:\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "peakrank=driver.find_elements(By.XPATH,'//div[@class=\"chart-results-list // lrv-u-padding-t-150 lrv-u-padding-t-050@mobile-max\"]/div/ul/li[4]/ul/li[5]/span')\n",
    "for i in peakrank:\n",
    "    Peak_rank.append(i.text)\n",
    "    \n",
    "weekboard=driver.find_elements(By.XPATH,'//div[@class=\"chart-results-list // lrv-u-padding-t-150 lrv-u-padding-t-050@mobile-max\"]/div/ul/li[4]/ul/li[6]/span')\n",
    "for i in weekboard:\n",
    "    Weeks_on_board.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e23bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Song_Name),len(Artist_name),len(Last_week_rank),len(Peak_rank),len(Weeks_on_board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e617e6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks On Board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fukumean</td>\n",
       "      <td>Gunna</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Lagunas</td>\n",
       "      <td>Peso Pluma &amp; Jasiel Nunez</td>\n",
       "      <td>-</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Overdrive</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bzrp Music Sessions, Vol. 55</td>\n",
       "      <td>Bizarrap &amp; Peso Pluma</td>\n",
       "      <td>99</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Dawns</td>\n",
       "      <td>Zach Bryan Featuring Maggie Rogers</td>\n",
       "      <td>-</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Rubicon</td>\n",
       "      <td>Peso Pluma</td>\n",
       "      <td>-</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Song Name                         Artist Name  \\\n",
       "0                     Last Night                       Morgan Wallen   \n",
       "1                       Fast Car                          Luke Combs   \n",
       "2                   Cruel Summer                        Taylor Swift   \n",
       "3                      Calm Down                 Rema & Selena Gomez   \n",
       "4                       Fukumean                               Gunna   \n",
       "..                           ...                                 ...   \n",
       "95                       Lagunas           Peso Pluma & Jasiel Nunez   \n",
       "96                     Overdrive                         Post Malone   \n",
       "97  Bzrp Music Sessions, Vol. 55               Bizarrap & Peso Pluma   \n",
       "98                         Dawns  Zach Bryan Featuring Maggie Rogers   \n",
       "99                       Rubicon                          Peso Pluma   \n",
       "\n",
       "   Last Week Rank Peak Rank Weeks On Board  \n",
       "0               1         1             28  \n",
       "1               2         2             20  \n",
       "2               4         3             14  \n",
       "3               6         3             49  \n",
       "4               7         4              8  \n",
       "..            ...       ...            ...  \n",
       "95              -        77              6  \n",
       "96             68        47              3  \n",
       "97             99        31             10  \n",
       "98              -        42             15  \n",
       "99              -        63              6  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5=pd.DataFrame({\"Song Name\":Song_Name,\"Artist Name\":Artist_name,\"Last Week Rank\":Last_week_rank,\"Peak Rank\":Peak_rank,\"Weeks On Board\":Weeks_on_board})\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db648d",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of Highest selling novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fea0e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By #importing inbuilt class By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fedbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the automeated webpage\n",
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e79c55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the required details\n",
    "Book_name=[]\n",
    "Author_name=[]\n",
    "Volumes_sold=[]\n",
    "Publisher=[]\n",
    "Genre=[]\n",
    "\n",
    "#Scrapping the name of the book\n",
    "name=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[2]')\n",
    "for i in name:\n",
    "    Book_name.append(i.text)\n",
    "    \n",
    "#Scrapping the name of the Author    \n",
    "author=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[3]')\n",
    "for i in author:\n",
    "    Author_name.append(i.text)\n",
    "    \n",
    "#Scrapping the Volumme solds \n",
    "volume=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[4]')\n",
    "for i in volume:\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "#Scrapping the Publisher name\n",
    "publisher=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[5]')\n",
    "for i in publisher:\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "#Scrapping the Genre\n",
    "genre=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[6]')\n",
    "for i in genre:\n",
    "    Genre.append(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfcf7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Book_name),len(Author_name),len(Volumes_sold),len(Publisher),len(Genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79ba550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes Sold</th>\n",
       "      <th>Publisher Name</th>\n",
       "      <th>Genre of Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name       Author Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes Sold   Publisher Name                Genre of Book  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6=pd.DataFrame({\"Book Name\":Book_name,\"Author Name\":Author_name,\"Volumes Sold\":Volumes_sold,\"Publisher Name\":Publisher,\"Genre of Book\":Genre})\n",
    "df6\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be1546",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614d198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By #importing inbuilt class By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1daabfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the automated page from the driver\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfc5476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the required details \n",
    "\n",
    "Name=[]\n",
    "Year_span=[]\n",
    "Genre=[]\n",
    "Run_time=[]\n",
    "Ratings=[]\n",
    "Votes=[]\n",
    "\n",
    "#Scrapping the name of the Series\n",
    "name=driver.find_elements(By.XPATH,'//*[@class=\"lister-item-content\"]/h3/a')\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "#Scrapping the span of the series    \n",
    "span=driver.find_elements(By.XPATH,'//*[@class=\"lister-item-content\"]/h3/span[2]')\n",
    "for i in span:\n",
    "    Year_span.append(i.text)\n",
    "\n",
    "#Scrapping the genre of the series    \n",
    "genre=driver.find_elements(By.XPATH,'//*[@class=\"lister-item-content\"]/p[1]/span[5]')\n",
    "for i in genre:\n",
    "    Genre.append(i.text)\n",
    "\n",
    "#Scrapping the runtime of the Series    \n",
    "runtime=driver.find_elements(By.XPATH,'//*[@class=\"lister-item-content\"]/p[1]/span[3]')\n",
    "for i in runtime:\n",
    "    Run_time.append(i.text)\n",
    "\n",
    "#Scrapping the rating of the Series    \n",
    "rating=driver.find_elements(By.XPATH,'//*[@class=\"lister-item-content\"]/div[1]/div[1]/span[2]')\n",
    "for i in rating:\n",
    "    Ratings.append(i.text)\n",
    "\n",
    "#Scrapping the Votes for the series    \n",
    "vote=driver.find_elements(By.XPATH,'//*[@class=\"lister-item-content\"]/p[4]/span[2]')\n",
    "for i in vote:\n",
    "    Votes.append(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00c63bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Name),len(Year_span),len(Genre),len(Run_time),len(Ratings),len(Votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cdcb1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of the Series</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,194,104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2024)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,267,722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,041,432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>306,099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>265,173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>52,469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>64,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>210,185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>43,705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>263,859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name of the Series    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things  (2016–2024)    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds     (2005– )     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "   Run time Ratings      Votes  \n",
       "0    57 min     9.2  2,194,104  \n",
       "1    51 min     8.7  1,267,722  \n",
       "2    44 min     8.1  1,041,432  \n",
       "3    60 min     7.5    306,099  \n",
       "4    43 min     7.6    265,173  \n",
       "..      ...     ...        ...  \n",
       "95   42 min     7.4     52,469  \n",
       "96   50 min     7.8     64,490  \n",
       "97   42 min     8.1    210,185  \n",
       "98   45 min       7     43,705  \n",
       "99  572 min     8.6    263,859  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7=pd.DataFrame({\"Name of the Series\":Name,\"Year Span\":Year_span,\"Genre\":Genre,\"Run time\":Run_time,\"Ratings\":Ratings,\"Votes\":Votes})\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93ad1b",
   "metadata": {},
   "source": [
    "# 8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93bb1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import required libraries\n",
    "import selenium                             #library that is used to work with selenium\n",
    "from selenium import webdriver              #importing webdriver module from selenium to open automated chrome window\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By #importing inbuilt class By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                 #use to stop search engine for few seconds\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#for handling the exceptions while scrapping\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c6891aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the automated page from the driver\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "\n",
    "#clicking on the button on the homepage for viewing all datasets\n",
    "dataset_button=driver.find_element(By.XPATH,'//a[@class=\"btn-primary btn\"]')\n",
    "dataset_button.click()\n",
    "\n",
    "#clicking on the expand all button to view all details for scrapping\n",
    "expandall_button=driver.find_element(By.XPATH,'//span[@class=\"swap-on text-primary-content\"]')\n",
    "expandall_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b364180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the details of the page required\n",
    "\n",
    "Dataset_name=[]\n",
    "Data_type=[]\n",
    "Task=[]\n",
    "Attribute_type=[]\n",
    "No_of_instances=[]\n",
    "NO_of_attribute=[]\n",
    "Year=[]\n",
    "\n",
    "#scraping the dataset name\n",
    "dataset=driver.find_elements(By.XPATH,'//div[@class=\"relative col-span-8 sm:col-span-7\"]/h2/a')\n",
    "for i in dataset:\n",
    "    Dataset_name.append(i.text)\n",
    "\n",
    "#scrapping the Data type    \n",
    "data_type=driver.find_elements(By.XPATH,'//div[@class=\"relative col-span-8 sm:col-span-7\"]/div/div[2]/span')\n",
    "for i in data_type:\n",
    "    Data_type.append(i.text)\n",
    "    \n",
    "#scrapping the task    \n",
    "task=driver.find_elements(By.XPATH,'//div[@class=\"relative col-span-8 sm:col-span-7\"]/div/div[1]/span')\n",
    "for i in task:\n",
    "    Task.append(i.text)\n",
    "    \n",
    "#scrapping the Attribute type    \n",
    "attribute=driver.find_elements(By.XPATH,'//table[@class=\"col-span-full my-2 table sm:col-start-2\"]/tbody/tr/td[2]')\n",
    "for i in attribute:\n",
    "    Attribute_type.append(i.text)\n",
    "    \n",
    "#Scrapping the No of instances   \n",
    "instances=driver.find_elements(By.XPATH,'//div[@class=\"relative col-span-8 sm:col-span-7\"]/div/div[3]/span')\n",
    "for i in instances:\n",
    "    No_of_instances.append(i.text)\n",
    "    \n",
    "#scrapping the No of Attributes    \n",
    "No_attribute=driver.find_elements(By.XPATH,'//div[@class=\"relative col-span-8 sm:col-span-7\"]/div/div[4]/span')\n",
    "for i in No_attribute:\n",
    "    NO_of_attribute.append(i.text)\n",
    "\n",
    "#scrapping the year of the dataset    \n",
    "year_of_dataset=driver.find_elements(By.XPATH,'//table[@class=\"col-span-full my-2 table sm:col-start-2\"]/tbody/tr/td[3]')\n",
    "for i in year_of_dataset:\n",
    "    Year.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "867fc63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Dataset_name),len(Data_type),len(Task),len(Attribute_type),len(No_of_instances),len(NO_of_attribute),len(Year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "686284de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>NO of Instances</th>\n",
       "      <th>NO of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150 Instances</td>\n",
       "      <td>4 Attributes</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303 Instances</td>\n",
       "      <td>13 Attributes</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Attributes</td>\n",
       "      <td>5/1/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13.61K Instances</td>\n",
       "      <td>16 Attributes</td>\n",
       "      <td>9/14/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td></td>\n",
       "      <td>20 Attributes</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>178 Instances</td>\n",
       "      <td>13 Attributes</td>\n",
       "      <td>7/1/1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>569 Instances</td>\n",
       "      <td>30 Attributes</td>\n",
       "      <td>11/1/1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Car Evaluation</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>1.73K Instances</td>\n",
       "      <td>6 Attributes</td>\n",
       "      <td>6/1/1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>3.81K Instances</td>\n",
       "      <td>8 Attributes</td>\n",
       "      <td>10/6/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>8.12K Instances</td>\n",
       "      <td>22 Attributes</td>\n",
       "      <td>4/27/1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Dataset Name     Data Type            Task  \\\n",
       "0                                  Iris  Multivariate  Classification   \n",
       "1                         Heart Disease  Multivariate  Classification   \n",
       "2                                 Adult  Multivariate  Classification   \n",
       "3                      Dry Bean Dataset  Multivariate  Classification   \n",
       "4                              Diabetes                                 \n",
       "5                                  Wine  Multivariate  Classification   \n",
       "6  Breast Cancer Wisconsin (Diagnostic)  Multivariate  Classification   \n",
       "7                        Car Evaluation  Multivariate  Classification   \n",
       "8            Rice (Cammeo and Osmancik)  Multivariate  Classification   \n",
       "9                              Mushroom  Multivariate  Classification   \n",
       "\n",
       "               Attribute Type   NO of Instances NO of Attributes       Year  \n",
       "0                        Real     150 Instances     4 Attributes   7/1/1988  \n",
       "1  Categorical, Integer, Real     303 Instances    13 Attributes   7/1/1988  \n",
       "2        Categorical, Integer  48.84K Instances    14 Attributes   5/1/1996  \n",
       "3               Integer, Real  13.61K Instances    16 Attributes  9/14/2020  \n",
       "4        Categorical, Integer                      20 Attributes        N/A  \n",
       "5               Integer, Real     178 Instances    13 Attributes   7/1/1991  \n",
       "6                        Real     569 Instances    30 Attributes  11/1/1995  \n",
       "7                 Categorical   1.73K Instances     6 Attributes   6/1/1997  \n",
       "8                        Real   3.81K Instances     8 Attributes  10/6/2019  \n",
       "9                 Categorical   8.12K Instances    22 Attributes  4/27/1987  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8=pd.DataFrame({\"Dataset Name\":Dataset_name,\"Data Type\":Data_type,\"Task\":Task,\"Attribute Type\":Attribute_type,\"NO of Instances\":No_of_instances,\"NO of Attributes\":NO_of_attribute,\"Year\":Year})\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b83b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
